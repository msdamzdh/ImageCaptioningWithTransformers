{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Image Captioning Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_labels['tokenized'] = self.img_labels.iloc[:,1].apply(lambda x: wordpunct_tokenize(x))\n",
    "        self.img_labels['len+2'] = self.img_labels.iloc[:,2].apply(lambda x: len(x)+2) #2 added for <SOS> and <EOS>\n",
    "        self.max_sentence_length = int(self.img_labels['len+2'].max())\n",
    "        all_tokens = [token for tokens in self.img_labels['tokenized'] for token in tokens]\n",
    "        unique_tokens = set(all_tokens)\n",
    "        self.vocab_size = len(unique_tokens)+3\n",
    "        self.img_labels['tokenized'] = self.img_labels['tokenized'].apply(lambda x: ['<SOS>']+x+['<EOS>'])\n",
    "        self.word_to_idx = {'<PAD>':0,'<SOS>':1,'<EOS>':2}\n",
    "        self.word_to_idx.update({word:idx for idx,word in enumerate(unique_tokens,start=3)})\n",
    "        self.idx_to_word = {value:key for key,value in self.word_to_idx.items()}\n",
    "        self.img_labels['sent_to_idx'] = self.img_labels['tokenized'].apply(lambda x: [self.word_to_idx.get(z) for z in x])\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels['sent_to_idx'].get(idx)\n",
    "        pad_length = self.max_sentence_length-len(self.img_labels['sent_to_idx'].get(idx))\n",
    "        label = label+pad_length*[0]\n",
    "        label = torch.tensor(label)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tgt_key_padding_mask = label[:-1]==0\n",
    "        return image, label, tgt_key_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Extraction using CNN (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)  # Load pre-trained ResNet\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False  # Freeze ResNet layers\n",
    "        \n",
    "        # Remove the final layer and add an embedding layer\n",
    "        resnet_layers_except_last = list(resnet.children())[:-1] # this is a list of layers\n",
    "        self.resnet = nn.Sequential(*resnet_layers_except_last)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)  # Extract features\n",
    "        features = features.view(features.size(0), -1)  # Flatten features\n",
    "        features = self.fc(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Caption Generation using Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, num_layers,max_seq_len,device):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "        self.tgt_mask = nn.Transformer.generate_square_subsequent_mask(max_seq_len-1).to(device)\n",
    "    \n",
    "    def forward(self, features, captions,tgt_key_padding_mask,tgt_mask=None):\n",
    "        embeddings = self.embedding(captions)\n",
    "        if tgt_mask is None:\n",
    "            transformer_output = self.transformer(features.unsqueeze(1),\n",
    "                                                embeddings,\n",
    "                                                tgt_mask = self.tgt_mask,\n",
    "                                                tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        else:\n",
    "            transformer_output = self.transformer(features.unsqueeze(1),\n",
    "                                                embeddings,\n",
    "                                                tgt_mask = tgt_mask,\n",
    "                                                tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "        outputs = self.fc(transformer_output)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(encoder, decoder, loss_fn,dataloader, num_epochs, learning_rate, vocab_size,device):\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=learning_rate)\n",
    "    loss_vec = []\n",
    "    for epoch in range(num_epochs):\n",
    "        counter, loss_val= 0, 0\n",
    "        for images, captions, tgt_key_padding_mask in dataloader:\n",
    "            images, captions, tgt_key_padding_mask = images.to(device), captions.to(device), tgt_key_padding_mask.to(device)\n",
    "            # this implementation is done for one word prediction at time\n",
    "            in_captions = captions[:,:-1] \n",
    "            out_captions = captions[:,1:]\n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, in_captions, tgt_key_padding_mask)\n",
    "\n",
    "            # Compute loss and backpropagate\n",
    "            loss = loss_fn(outputs.reshape(-1, vocab_size), out_captions.reshape(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_val = loss_val+loss.item()\n",
    "            counter+=1\n",
    "        loss_in_epoch = loss_val/counter\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss_in_epoch}\")\n",
    "        loss_vec.append(loss_in_epoch)\n",
    "    return loss_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Initialize the model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0005\n",
    "num_layers = 2\n",
    "batch_size = 16\n",
    "\n",
    "Image_Dir = 'Path//to//image_folder'\n",
    "Annot_Dir = 'Path//to//csv_file'\n",
    "\n",
    "composer = transforms.Compose([transforms.ToPILImage(),\n",
    "                               transforms.Resize([256,256]),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(0,1)])\n",
    "\n",
    "full_dataset = CustomImageDataset(Annot_Dir,Image_Dir,transform=composer)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [0.8, 0.2])\n",
    "\n",
    "vocab_size = full_dataset.vocab_size\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = CNNEncoder(embed_size).to(device)\n",
    "decoder = TransformerCaptioningModel(embed_size, vocab_size, hidden_size, num_layers,full_dataset.max_sentence_length,device).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "# Load data using a DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "loss_vec = train_model(encoder, decoder, loss_fn,train_dataloader, num_epochs, learning_rate, vocab_size, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "line_up, = ax.plot(range(1,len(loss_vec)+1),loss_vec, label='Line 1')\n",
    "ax.legend([line_up], ['Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Give image and get caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_Address = 'Path//to//sigle_image'\n",
    "Image = mpimg.imread(Image_Address)\n",
    "plt.imshow(Image)\n",
    "plt.axis('off')\n",
    "# read image\n",
    "image = read_image(Image_Address)\n",
    "\n",
    "resized_image = composer(image).unsqueeze(0).to(device)\n",
    "features = encoder(resized_image)\n",
    "first_word = torch.ones([1,1],dtype=torch.int64).to(device) #<SOS>\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.inference_mode():\n",
    "    generated_caption = first_word.to(device)\n",
    "    special_idx = [0,1,2]\n",
    "    cond = True\n",
    "    while cond:         \n",
    "        tgt_key_padding_mask=generated_caption==0\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(generated_caption.size(1)).to(device)            \n",
    "        output = decoder(features,generated_caption,tgt_key_padding_mask,tgt_mask)\n",
    "        new_caption = torch.argmax(output,dim=2)\n",
    "        new_caption = new_caption[:,-1].unsqueeze(1)\n",
    "        generated_caption = torch.hstack((generated_caption,new_caption))\n",
    "        idx_list = generated_caption.squeeze().tolist()\n",
    "        last_idx = idx_list[-1]\n",
    "        if (last_idx in special_idx) or (len(idx_list)==(full_dataset.max_sentence_length-1)):\n",
    "            cond = False\n",
    "        \n",
    "token = [full_dataset.idx_to_word.get(x) for x in idx_list]\n",
    "title = ' '.join([x for x in token if (x !='<PAD>' and x!='<SOS>' and x!='<EOS>')])\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
